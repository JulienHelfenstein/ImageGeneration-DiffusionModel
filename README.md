# Quick Diffusion â€” README

Ce dÃ©pÃ´t contient un script Python simple et autonome (`main.py`) implÃ©mentant un petit modÃ¨le de diffusion (U-Net simplifiÃ©) pour entraÃ®ner et gÃ©nÃ©rer des images Ã  partir d'un dataset local de type CelebA.

## ğŸ¯ Objectif

Documenter `main.py` :
- comment prÃ©parer les donnÃ©es localement ;
- quelles sont les dÃ©pendances ;
- comment lancer l'entraÃ®nement ;
- oÃ¹ retrouver les sorties (checkpoints & images prÃ©visualisÃ©es).

## ğŸ§­ RÃ©sumÃ© de `main.py`

- Le script crÃ©e un dataset local (`LocalCelebADataset`) qui cherche rÃ©cursivement des images `.jpg` (ou `.png` si none found) dans le dossier `data/`.
- Un modÃ¨le simple de type U-Net temporel (`QuickDiffusionUNet`) est dÃ©fini avec un bloc temporel (`TimeBlock`) pour injecter l'embedding du pas temporel.
- L'entraÃ®nement ajoute progressivement du bruit aux images (fonction `forward_noise`) et apprend Ã  prÃ©dire un niveau de bruit suivant pour chaque pas.
- Pendant l'entraÃ®nement, des checkpoints sont sauvegardÃ©s dans `checkpoints/` et une prÃ©visualisation globale est sauvegardÃ©e dans `results/` (images d'exemple par Ã©poque).
- Le script dÃ©tecte automatiquement l'appareil : GPU CUDA, MPS (Mac M1/M2/M3) ou CPU.

## âš™ï¸ HyperparamÃ¨tres et options (dÃ©finis en haut de `main.py`)
- IMG_SIZE = 64 â€” taille (HÃ—W) des images trainÃ©es
- BATCH_SIZE = 64
- TIMESTEPS = 16 â€” nombre de pas de diffusion
- LR = 1e-4 â€” learning rate
- EPOCHS = 10

Ces variables peuvent Ãªtre modifiÃ©es directement dans `main.py` pour expÃ©rimenter.

## ğŸ“ Structure attendue du projet

- data/  <-- placez vos images ici (ex: `img_align_celeba/`)
- checkpoints/  <-- crÃ©Ã© automatiquement par le script (sauvegarde `.pth`)
- results/  <-- crÃ©Ã© automatiquement (sauvegarde `epoch_N.png`)

Remarque : Evitez d'ajouter `data/`, `*.pth` ou `results/` au dÃ©pÃ´t Git â€” `.gitignore` a Ã©tÃ© ajoutÃ© pour ces fichiers.

## âœ… DÃ©pendances

Le script utilise (extrait depuis `main.py`):
- Python 3.8+ (recommandÃ©)
- torch
- torchvision
- numpy
- pillow (PIL)
- matplotlib
- tqdm

Exemple d'installation :

```powershell
pip install torch torchvision numpy pillow matplotlib tqdm
```

Si vous utilisez un GPU, installez la version de `torch` compatible avec votre CUDA.

## ğŸš€ Comment lancer l'entraÃ®nement

1. Mettez vos images dans le dossier `data/` (ou un sous-dossier : script cherche rÃ©cursivement `*.jpg` / `*.png`).
2. (Optionnel) ajustez les hyperparamÃ¨tres en tÃªte de `main.py`.
3. ExÃ©cutez :

```powershell
python main.py
```

Remarques :
- Sur Windows, `DataLoader` utilise `num_workers=0` pour Ã©viter des erreurs de multiprocessing. Sur Linux/Mac vous pouvez augmenter `num_workers`.
- Si vous sentez des problÃ¨mes de mÃ©moire, rÃ©duisez `BATCH_SIZE` ou `IMG_SIZE`.

## ğŸ’¾ Sorties / Checkpoints

- `checkpoints/model_ep{N}.pth` â€” Ã©tats du modÃ¨le enregistrÃ©s aprÃ¨s chaque Ã©poque
- `results/epoch_{N}.png` â€” grille d'images gÃ©nÃ©rÃ©es en fin d'Ã©poque (prÃ©visualisation)

## ğŸ“Œ Astuces et suggestions

- Les modÃ¨les et datasets peuvent Ãªtre volumineux : si vous souhaitez suivre les `.pth` dans Git, configurez Git LFS (`git lfs track "*.pth"`) pour Ã©viter d'avoir de gros fichiers git historiques.
- Si vous avez accidentellement committÃ© de gros fichiers, je peux vous aider Ã  les supprimer de l'historique (avec `git filter-repo` ou `bfg`).

## â“ Prochaine Ã©tape â€” amÃ©lioration possible

- Ajouter un fichier `requirements.txt` pour simplifier l'installation.
- Ajouter des scripts CLI pour configurer hyperparamÃ¨tres via des flags.
- Ajouter un petit notebook pour visualiser les images gÃ©nÃ©rÃ©es et l'Ã©volution du training.

---

Si tu veux, je peux maintenant :
- ajouter `requirements.txt`,
- configurer Git LFS pour les `.pth`,
- ou commit & push ce `README.md` sur ton repo (je peux faire Ã§a tout de suite).